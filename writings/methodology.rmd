# Methodology

There are two topics that I want to address:

1. Do Portuguese private schools outperform public schools because of
the student's characteristics? Or because they are of higher (average)
quality relative to comparable schools?

2. In general, how much grade inflation is there? What are the best
predictors of that inflation?

In the first one, there are some very large difficulties in answering the question
directly. Some important data is missing (which has to be imputed), and the
causal effects cannot inform us about school quality as such. Instead, it
informs us about the "local relative quality" -- if a school were changed from public
to private and the students were the same, how would their grades change? There are
some possible confounding variables, but I argue that those should mostly be
favoring private schools. Thus, it is likely that this analysis can at
least give me higher bounds for the differences between those types of schools.

For the second question, I can compare the school's average grade
in national standardized exams with the school's internal grade
and assume that discrepancies are due to *relative* grade inflation.
In this case, there is also missing data: students with a grade smaller
than 10 (the minimum grade to pass) do not have their internal grade reported,
but they still answer the national exams.

I first fit the models on simulated data to show that the models are identifiable
from a causal perspective, and that my sample has enough power to detect differences.
I also do it for practical considerations. These models are hard to fit
(for different reasons), and that forced me to experiment a lot.
To avoid data snooping, that experimentation was done on the fake data.
This workflow is more extensively shown in the
[Bayesian Workflow article](https://arxiv.org/pdf/2011.01808.pdf).
My only complaint about that article is that they seem a bit too comfortable
pushing for iterative model development and model checking on the real data.
To me, it seems better to fit the most "expanded" model I can -- a model that
can represent all simpler models as a special case -- and in the end change the
priors as a kind of robustness check that is actually useful.
(Model expansion as an end-goal was more emphasized
[here](http://www.stat.columbia.edu/~gelman/research/published/philosophy.pdf).)

The structure of the remaining methodology is the following:

1. Causal models. The assumptions that allow for identifiability, and their limitations.
2. Statistical models. Here I want to prove that given all the other issues we see
in the real data -- measurement error, non-linear relationships, interactions, and
missing data -- we can still fit the model and estimate the parameters with
enough power.
Those simulations are done in the `/model_validation/generate_posterior_samples.py` file.
That file outputs all the MCMC chains to `/model_validation/posterior_samples`,
and those chains are evaluated in `/model_validation/check_posterior_samples.rmd`.
The real data will be fit on the last model, which is the most complete one.
That is also where I will change the priors to see how a relatively simple
or complex model would change the results.

## Causal models

### Causal model for school outperformance

```{r}
library(dagitty)
final_dag <- dagitty(x='dag {
AvgParentEdu [pos="-0.335,-0.862"]
GradesExam [outcome,pos="-0.867,-0.356"]
IsPrivateSchool [exposure,pos="-1.350,-0.860"]
PctSES [pos="-0.464,-1.341"]
SpatialEffects [pos="-1.131,-1.344"]
AvgParentEdu -> GradesExam
AvgParentEdu -> PctSES
IsPrivateSchool -> AvgParentEdu
IsPrivateSchool -> GradesExam
IsPrivateSchool -> PctSES
PctSES -> GradesExam
SpatialEffects -> AvgParentEdu
SpatialEffects -> GradesExam
SpatialEffects -> IsPrivateSchool
SpatialEffects -> PctSES
}')
plot(final_dag)
print(impliedConditionalIndependencies(final_dag))
print(dagitty::adjustmentSets(final_dag, effect = 'total'))
print(dagitty::adjustmentSets(final_dag, effect = 'direct'))
```

#### Explanation for the model

* The "spatial effects" stand for all the unobservable variables that are
correlated in space, like the economic and social conditions of people within
a certain location. Those conditions may affect the quality of the schools,
so the estimated difference in performance between schools is local/conditional
(for example, if the difference between public and private schools is 0.5, that
means that a public school

* Private schools cause a higher percentage of students of higher socioeconomic
status (SES), not the other way around. As a proxy for SES, I use the percentage
of 12th-grade students in a given school that receive financial support.

* Private schools may also attract parents with higher education, even after
adjusting for other variables, because those parents might value education more
highly. Higher education of the parents will also cause a higher SES.
(These two assumptions are a bit flimsy but not important, as they do
not change what I should control for.) As a proxy for education, I use the average
education of the parents of 12th-grade students in a given school.

#### The main assumptions that break the model

* An unobserved variable that increases the probability of a high school or
a variable is increased by a school being private, and simulataneously causes
grades. An example of this would be that
private schools select for more interested parents given that they are spending
more money on education. Another example is that private schools explicitly select
students and parents with better characteristics.

* An unobserved variable that causes a higher percentage of students with low
socio-economic status or with low-education parents, and also causes grades.
This is a confounder. If it is not a confounder (i.e., this variable is *caused*
by a higher percentage of students), the variable is not problematic.

These problems would cause major issues if they were not correlated
with the school location. It seems that most variables -- income, crime, pollution,
additional costs of going to school, culture, etc. -- could plausibly be
captured by the location of the school.

For the variables where that is not true, it seems plausible that they would
cause a positive association between private schools and higher
grades. Additionally, to the extent that the variables I use are bad or noisy proxies
for unobservables, they would also cause a higher positive association between
private schools and high grades, so I can at least estimate a higher bound for
the estimated effect.

One last challenge is that the proxies have missing data for all private schools.
My model imputes that data based on the location of the school,
but those values have to be reasonable in order for the final inference to be reasonable.

### Causal model for school grade inflation

Given what I supposed in the other model, I assume that the external exams
are a good proxy for the 'true' grade of a student, so they are an accurate
measure of *relative* grade inflation (not necessarily absolute, as the exams can be
artificially harder than they should be).
I have 400.000+ observations for each year,
so using the other aggregate variables (such as the average parents' education in a school)
should not help me much when estimating the missing grades. This also helps me
fit the models in practice because even without any predictors, I am already dealing with a
few million observations, and each one of those will need its own parameter in order to
estimate the missing data.

In all schools, internal grades tend to be inflated relative
to exam grades. It seems that private schools do it more,
but there is a plausible mechanism by which grade inflation gets
*underestimated*: an internal grade below 10 is not recorded,
so the average of the observed internal grades will be more inflated
as the number of students with negative grades increases. Here is a small
simulation showing this phenomenon:

```{r}
library(tidyverse)
df <- data.frame(row.names = 1:1500) %>%
    mutate(exam_grade = rnorm(50*30, mean=10, sd=3),  # assuming unbounded grade
           internal_grade_true = rnorm(50*30, mean=exam_grade + 1, sd=1),
           school = rep(1:50, each=30)) %>%
    group_by(school) %>%
    summarise(avg_grade_exam = mean(exam_grade),
              avg_grade_int_true = mean(internal_grade_true),
              avg_grade_int_false = mean(internal_grade_true[internal_grade_true > 10]),
              n_exc = NROW(internal_grade_true[internal_grade_true < 10]))

df %>%
    ggplot(aes(avg_grade_int_true, log(avg_grade_int_true/avg_grade_exam))) +
    geom_point() +
    geom_abline() +
    geom_smooth(method ='lm') +
    geom_point(aes(avg_grade_int_true, log(avg_grade_int_false/avg_grade_exam)), color='red') +
    geom_smooth(aes(avg_grade_int_true, log(avg_grade_int_false/avg_grade_exam)), color='red', method='lm')
```

The blue line shows that internal grades have a grade inflation of roughly
10% (y-axis), independently of the internal grade of the school (x-axis).
But the red line shows that, due to only averaging positive grades,
all schools look like they inflate grades more, wit this being
especially true for schools with lower grades (x-axis).
The effect is substantially smaller if the average grade is not so close to 10
but, above all, this issue causes a large amount of measurement error when
there are few students taking the exam.

## Statistical models

Here I only detail how the final model works and its assumptions. Every
other model was fit more so as a way of 'unit testing' each component of
the final model.

TODO: continue
